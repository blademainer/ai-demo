# å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹Transformerä¸­è‹±æ–‡ç¿»è¯‘ç³»ç»Ÿã€‚

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
cd llm-translate
pip install -r requirements.txt
```

### 2. éªŒè¯ç¯å¢ƒ

```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
```

## å¿«é€Ÿæ¼”ç¤ºï¼ˆä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰

### æ­¥éª¤1: åˆ›å»ºç¤ºä¾‹æ•°æ®

```bash
python scripts/create_sample_data.py
```

è¿™å°†åœ¨`data/raw/`ç›®å½•ä¸‹åˆ›å»ºç¤ºä¾‹çš„ä¸­è‹±æ–‡å¹³è¡Œè¯­æ–™ã€‚

### æ­¥éª¤2: æ•°æ®é¢„å¤„ç†

```bash
python scripts/prepare_data.py \
    --train-src data/raw/train.zh \
    --train-tgt data/raw/train.en \
    --val-src data/raw/val.zh \
    --val-tgt data/raw/val.en \
    --test-src data/raw/test.zh \
    --test-tgt data/raw/test.en \
    --vocab-size 8000
```

é¢„å¤„ç†åŒ…æ‹¬ï¼š
- æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–
- è®­ç»ƒSentencePieceåˆ†è¯å™¨
- ç”Ÿæˆè¯æ±‡è¡¨

é¢„å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨ï¼š
- `data/processed/` - å¤„ç†åçš„æ–‡æœ¬
- `data/vocab/` - åˆ†è¯å™¨æ¨¡å‹

### æ­¥éª¤3: é…ç½®è°ƒæ•´ï¼ˆå¯é€‰ï¼‰

å¯¹äºæ¼”ç¤ºï¼Œå»ºè®®è°ƒæ•´é…ç½®ä»¥åŠ å¿«è®­ç»ƒï¼š

ç¼–è¾‘ `configs/training_config.yaml`:
```yaml
training:
  num_epochs: 10          # å‡å°‘epochæ•°
  batch_size: 32          # æ ¹æ®æ˜¾å­˜è°ƒæ•´
  eval_interval: 100      # æ›´é¢‘ç¹éªŒè¯
  save_interval: 500      # æ›´é¢‘ç¹ä¿å­˜
```

ç¼–è¾‘ `configs/model_config.yaml`:
```yaml
model:
  src_vocab_size: 8000    # ä¸prepare_dataçš„vocab_sizeä¸€è‡´
  tgt_vocab_size: 8000
  d_model: 256            # å‡å°æ¨¡å‹ä»¥åŠ å¿«è®­ç»ƒ
  num_encoder_layers: 3   # å‡å°‘å±‚æ•°
  num_decoder_layers: 3
  d_ff: 1024
```

### æ­¥éª¤4: å¼€å§‹è®­ç»ƒ

```bash
python scripts/train.py
```

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šï¼š
- è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ°`checkpoints/`
- è®°å½•æ—¥å¿—åˆ°`logs/`
- åœ¨éªŒè¯é›†ä¸Šå®šæœŸè¯„ä¼°

### æ­¥éª¤5: ç›‘æ§è®­ç»ƒ

åœ¨å¦ä¸€ä¸ªç»ˆç«¯çª—å£è¿è¡Œï¼š

```bash
tensorboard --logdir logs
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹ï¼š
- è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- å­¦ä¹ ç‡å˜åŒ–
- å…¶ä»–è®­ç»ƒæŒ‡æ ‡

### æ­¥éª¤6: è¯„ä¼°æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼š

```bash
python scripts/evaluate.py \
    --checkpoint checkpoints/best_model.pt \
    --batch-size 32 \
    --output results.json
```

## ä½¿ç”¨çœŸå®æ•°æ®é›†

### æ¨èæ•°æ®é›†

1. **WMTç¿»è¯‘ä»»åŠ¡æ•°æ®**
   ```bash
   # ä¸‹è½½WMT19ä¸­è‹±ç¿»è¯‘æ•°æ®
   wget http://data.statmt.org/wmt19/translation-task/zh-en.tgz
   tar -xzf zh-en.tgz
   ```

2. **OPUSå¤šè¯­è¨€è¯­æ–™åº“**
   ```bash
   # ä½¿ç”¨opus-toolsä¸‹è½½
   pip install opustools-pkg
   opus_read -d OpenSubtitles -s zh -t en -w opus-data/opensubs.zh opus-data/opensubs.en
   ```

3. **AI Challenger 2017**
   - ä¸‹è½½åœ°å€: https://challenger.ai/dataset/translation

### æ•°æ®å‡†å¤‡æµç¨‹

```bash
# 1. å°†ä¸‹è½½çš„æ•°æ®æ”¾åœ¨data/raw/ç›®å½•
# 2. è¿è¡Œé¢„å¤„ç†
python scripts/prepare_data.py \
    --train-src data/raw/train.zh \
    --train-tgt data/raw/train.en \
    --val-src data/raw/val.zh \
    --val-tgt data/raw/val.en \
    --test-src data/raw/test.zh \
    --test-tgt data/raw/test.en \
    --vocab-size 32000

# 3. å¼€å§‹è®­ç»ƒ
python scripts/train.py
```

## è®­ç»ƒæŠ€å·§

### å°æ˜¾å­˜GPUï¼ˆ<8GBï¼‰

```yaml
# configs/training_config.yaml
training:
  batch_size: 16
  gradient_accumulation_steps: 4  # ç­‰æ•ˆbatch_size=64
  fp16: true
```

### ä¸­ç­‰æ˜¾å­˜GPUï¼ˆ8-16GBï¼‰

```yaml
training:
  batch_size: 32
  gradient_accumulation_steps: 2
  fp16: true
```

### å¤§æ˜¾å­˜GPUï¼ˆ>16GBï¼‰

```yaml
training:
  batch_size: 64
  gradient_accumulation_steps: 1
  fp16: true
```

### å¤šGPUè®­ç»ƒï¼ˆå¾…å®ç°ï¼‰

```bash
# ä½¿ç”¨PyTorch DDP
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    scripts/train.py
```

## æ¨ç†ç¤ºä¾‹

åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¿»è¯‘è„šæœ¬ï¼š

```python
# translate.py
import torch
import sentencepiece as spm
from src.model import Transformer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = torch.load('checkpoints/best_model.pt', map_location=device)
tokenizer = spm.SentencePieceProcessor()
tokenizer.load('data/vocab/spm_zh.model')

# ç¿»è¯‘å‡½æ•°
def translate(text):
    # ç¼–ç 
    src_ids = tokenizer.encode(text, add_bos=True, add_eos=True)
    src_tensor = torch.tensor([src_ids]).to(device)
    
    # æ¨ç†
    with torch.no_grad():
        # è¿™é‡Œéœ€è¦å®ç°è´ªå¿ƒè§£ç æˆ–beam search
        pass
    
    # è§£ç 
    translation = tokenizer.decode(output_ids)
    return translation

# ä½¿ç”¨
print(translate("äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚"))
```

## å¸¸è§é—®é¢˜

### Q: è®­ç»ƒlossä¸ä¸‹é™ï¼Ÿ

æ£€æŸ¥ï¼š
- å­¦ä¹ ç‡æ˜¯å¦å¤ªå°æˆ–å¤ªå¤§
- æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
- æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–

### Q: æ˜¾å­˜æº¢å‡ºï¼Ÿ

å°è¯•ï¼š
- å‡å°batch_size
- å‡å°max_seq_length
- å¯ç”¨fp16
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

### Q: æ¨ç†å¤ªæ…¢ï¼Ÿ

ä¼˜åŒ–ï¼š
- ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†
- å¯ç”¨KVç¼“å­˜
- æ¨¡å‹é‡åŒ–ï¼ˆINT8ï¼‰
- ä½¿ç”¨ONNXå¯¼å‡º

## ä¸‹ä¸€æ­¥

- é˜…è¯»å®Œæ•´çš„[README.md](README.md)
- æŸ¥çœ‹[DESIGN_DOCUMENT.md](DESIGN_DOCUMENT.md)äº†è§£æ¶æ„ç»†èŠ‚
- å®éªŒä¸åŒçš„è¶…å‚æ•°é…ç½®
- å°è¯•Beam Searchè§£ç 
- å®ç°æ³¨æ„åŠ›å¯è§†åŒ–
- å¯¼å‡ºæ¨¡å‹åˆ°ONNX

## è·å–å¸®åŠ©

å¦‚é‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—
2. æŸ¥çœ‹æ–‡æ¡£å’Œä»£ç æ³¨é‡Š
3. åœ¨GitHub Issuesä¸­æœç´¢ç±»ä¼¼é—®é¢˜
4. æäº¤æ–°çš„Issueå¹¶é™„ä¸Šè¯¦ç»†ä¿¡æ¯

ç¥æ‚¨è®­ç»ƒæ„‰å¿«ï¼ğŸš€
